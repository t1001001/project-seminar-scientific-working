\section{Methodology}

\subsection{Dataset and Preprocessing}
We use the LUNA16 chest CT benchmark derived from LIDC-IDRI~\cite{setio2017luna16}. Due to computational constraints, we operate on a subset of scans and extract two-dimensional axial slices from volumetric CTs. \\
Each slice is converted to PNG and has a size of $512 \times 512$; labels are generated in YOLO format with a single class (\texttt{nodule}). \\
To avoid randomness in the split, we use a deterministic partition, assigning 80\% of slices to training and 20\% to validation. \\
In total, preprocessing yields 70,315 slices.

\subsection{Label Generation}
World coordinates and nodule diameters (mm) are converted to voxel coordinates using scan-specific origin and spacing. For a slice index $z$, if a nodule center projects to that slice, its bounding box is constructed around the center using half the diameter along $x$ and $y$, normalized by slice width and height to produce YOLO labels $(x_c, y_c, w, h)$. Multiple nodules per slice are appended as separate lines.

\subsection{Training Settings}
We compare four settings under identical optimization and reporting protocols:
\begin{enumerate}
  \item \textbf{Baseline}: YOLO11 trained on real slices only.
  \item \textbf{Baseline + Classical Augmentation}: real slices plus label-preserving intensity augmentations on training images.
  \item \textbf{Baseline + CycleGAN}: real training slices combined with synthetic images generated by CycleGAN.
  \item \textbf{Classical Augmentation + CycleGAN}: combination of classical augmentations and CycleGAN-generated images.
\end{enumerate}
All settings use the same YOLO hyperparameters: \texttt{yolo11n} weights, 100 epochs, input size $512 \times 512$, batch size 32.

\subsection{Classical Augmentation}
We apply label-preserving appearance augmentations to \emph{training} images only. Geometric transformations such as rotation, flipping, cropping or scaling are avoided to prevent spatial misalignment with bounding boxes~\cite{padilla2020metrics}. Specifically, we use:
\begin{itemize}
  \item Brightness jitter: scale $\in [0.50, 1.50]$: Multiplies pixel values. $<1$ darkens, $>1$ brightens. Simulates exposure changes.
  \item Contrast jitter: scale $\in [0.50, 1.50]$: Scales intensity spread. Low flattens, high enhances. Mimics contrast variability.
  \item Gaussian blur: radius $\in [0.50, 1.50]$: Gaussian convolution. Reduces high-frequency detail. Simulates smoothing/mild motion.
  \item Additive Gaussian noise: $\sigma \in [5, 15]$ (pixel intensity, clipped to [0,255]): Adds zero-mean noise. Emulates sensor/reconstruction noise.
  \item CLAHE (contrast-limited adaptive histogram equalization): clipLimit $\in [2.0, 3.0]$, tileGridSize $\in \{(8,8),(16,16)\}$: Local histogram equalization with clipping. Enhances local contrast while limiting noise amplification.
\end{itemize}
Each augmented copy applies exactly one of the aforementioned transformations. For each baseline image, five augmented images are generated, and labels are copied 1:1 from the original.

\subsection{CycleGAN Setup}
CycleGAN is used to introduce appearance-level variability, such as texture, intensity and contrast while preserving anatomical structure. \\
We construct two domains A (original) and B (styled) from the same slice set via a deterministic 80/20 split. \texttt{trainA/testA} contain original slices, while \texttt{trainB/testB} apply a fixed style transform (brightness 1.15, contrast 1.4) to form a target appearance domain. Samples are unpaired, consistent with CycleGAN’s design~\cite{zhu2017cyclegan}. \\
In total, all 70.315 slices were used to build the domains, yielding 56.252 trainA/trainB and 14.063 testA/testB images. Due to computational constraints, CycleGAN training is capped at 2.000 images instead of all images; this cap applies only to CycleGAN training. \\
CycleGAN is trained for 25 epochs plus 25 epochs of linear learning-rate decay (50 epochs total). At epoch 50, the generator produces synthetic outputs for domain A and B (fake, real and reconstructed). Only the \emph{fake} images are used to augment YOLO training; real and reconstructed outputs are discarded. Concretely, 300 synthetic outputs are produced, of which 100 fake images are incorporated into the training set (these 100 images constitute $N_{\text{gan}}$ in the results). When added to the YOLO training, synthetic files are saved with a \texttt{cyc\_*} prefix, and labels are copied from the corresponding real slice. \\
Because the A -> B translation only operates at the appearance level and preserves image geometry, we copy the label from the corresponding original slice to each generated image. \\
YOLO is trained on the full real training split. In training settings including CycleGAN, we add the 100 CycleGAN fake images; validation images remain unaugmented.

\subsection{Evaluation Protocol}
We evaluate all models on a fixed validation split using standard object detection metrics~\cite{padilla2020metrics}. Evaluations are performed with the Ultralytics validation routine. Hyperparameters and data splits are identical across all training regimes.
\paragraph{Metric Definitions}
\begin{itemize}
    \item \textbf{AP@0.5}: Average Precision computed as the area under the precision–recall curve at an Intersection over Union (IoU) threshold of 0.5.
    \item \textbf{AP@0.5:0.95}: Mean Average Precision (mAP) computed as the average of AP values over IoU thresholds from 0.50 to 0.95 in increments of 0.05 (i.e., 10 thresholds).
    \item \textbf{Mean IoU}: Mean Intersection over Union computed only over true positive detections with IoU $\geq 0.5$, using 1:1 matching between predictions and ground-truth boxes.
\end{itemize}

\subsection{Implementation Details}
All experiments use identical YOLO hyperparameters and the same data split to ensure comparability. CycleGAN follows the default architecture and losses (adversarial + cycle-consistency), except for the number of real images for training~\cite{zhu2017cyclegan}. The LUNA16 subset choice and 2D slice processing reflect computational constraints and the focus on pulmonary nodule detection in axial slices.