\section{Methodology}

\subsection{Dataset and Preprocessing}
We use the LUNA16 chest CT benchmark derived from LIDC-IDRI~\cite{setio2017luna16}. Due to computational constraints, we operate on a subset of scans and extract two-dimensional axial slices from volumetric CTs. Each slice is converted to PNG after Hounsfield unit clipping to $[-1000, 400]$ and linear normalization to 8-bit (0--255). Labels are generated in YOLO format with a single class (\texttt{nodule}). \\
To avoid randomness in the split, we use a deterministic partition, assigning 80\% of slices to training and 20\% to validation. 

\subsection{Label Generation}
World coordinates and nodule diameters (mm) are converted to voxel coordinates using scan-specific origin and spacing. For a slice index $z$, if a nodule center projects to that slice, its bounding box is constructed around the center using half the diameter along $x$ and $y$, normalized by slice width and height to produce YOLO labels $(x_c, y_c, w, h)$. Multiple nodules per slice are appended as separate lines.

\subsection{Training Settings}
We compare four settings under identical optimization and reporting protocols:
\begin{enumerate}
  \item \textbf{Baseline}: YOLOv11 trained on real slices only.
  \item \textbf{Baseline + Classical Augmentation}: real slices plus label-preserving intensity augmentations on training images.
  \item \textbf{Baseline + CycleGAN}: real training slices combined with synthetic images generated by CycleGAN.
  \item \textbf{Classical Augmentation + CycleGAN}: combination of classical augmentations and CycleGAN-generated images.
\end{enumerate}
All settings use the same YOLO hyperparameters: \texttt{yolo11n} weights, 100 epochs, input size $512 \times 512$, batch size 32.

\subsection{Classical Augmentation}
We apply label-preserving appearance augmentations to \emph{training} images only. Geometric transformations such as rotation, flipping, cropping or scaling are avoided to prevent spatial misalignment with bounding boxes~\cite{padilla2020metrics}.. Specifically, we use:
\begin{itemize}
  \item Brightness jitter: scale $\in [0.50, 1.50]$
  \item Contrast jitter: scale $\in [0.50, 1.50]$
  \item Gaussian blur: radius $\in [0.50, 1.50]$
  \item Additive Gaussian noise: $\sigma \in [3, 10]$ (pixel intensity, clipped to [0,255])
  \item CLAHE (contrast-limited adaptive histogram equalization): clipLimit $\in [2.0, 3.0]$, tileGridSize $\in \{(8,8),(16,16)\}$
\end{itemize}
Each augmented copy applies a single transform (e.g., contrast-only, brightness-only, blur-only, noise-only, CLAHE-only, or color-jitter-only), cycling through types per image as needed. For each baseline image, five augmented images are generated.

\subsection{CycleGAN Setup}
CycleGAN is used to introduce appearance-level variability, such as texture, intensity and contrast while preserving anatomical structure. \\
We construct two domains $\mathcal{A}$ (original) and $\mathcal{B}$ (styled) from the same slice set via a deterministic 80/20 split. \texttt{trainA/testA} contain original slices, while \texttt{trainB/testB} apply a fixed style transform (contrast 1.4, brightness 1.15) to form a target appearance domain. Samples are unpaired, consistent with CycleGAN’s design~\cite{zhu2017cyclegan}. \\
In total, 22.870 slices were used to build the domains, yielding 20.583 trainA/trainB and 2.287 testA/testB images after the 90/10 split. \\
CycleGAN is trained for 50 epochs plus 50 epochs of linear learning-rate decay. Synthetic images are generated at epoch 50 and added only to the training set of the settings that use them, pairing each synthetic slice name with the label of its corresponding real slice. Furthermore, due to computational constraints, CycleGAN training is capped at 2.000 images; this cap applies only to CycleGAN training. \\
YOLO is trained on the full training split of real slices. In augmentation settings, we include all available synthetic images and classical augmented copies. CycleGAN augments appearance and does not introduce new nodules; it preserves object geometry and spatial layouts.

\subsection{Evaluation Protocol}
We evaluate all models on a fixed validation split using standard object detection metrics~\cite{padilla2020metrics}. Evaluations are performed with the Ultralytics validation routine (YOLO11). Hyperparameters and data splits are identical across all training regimes.
\paragraph{Metric Definitions}
\begin{itemize}
    \item \textbf{AP@0.5}: Average Precision computed as the area under the precision–recall curve at an Intersection over Union (IoU) threshold of 0.5.
    \item \textbf{AP@0.5:0.95}: Mean Average Precision (mAP) computed as the average of AP values over IoU thresholds from 0.50 to 0.95 in increments of 0.05 (i.e., 10 thresholds).
    \item \textbf{Mean IoU}: Mean Intersection over Union computed only over true positive detections with IoU $\geq 0.5$, using 1:1 matching between predictions and ground-truth boxes.
\end{itemize}

\subsection{Implementation Details}
All experiments use identical YOLO hyperparameters and the same data split to ensure comparability. CycleGAN follows the default architecture and losses (adversarial + cycle-consistency), except for the number of real images for training~\cite{zhu2017cyclegan}. The LUNA16 subset choice and 2D slice processing reflect computational constraints and the focus on pulmonary nodule detection in axial slices.

% \subsection{Limitations}
% Our study is limited by: (i) the use of a LUNA16 subset and 2D slices, which may omit inter-slice context; (ii) absence of a separate patient-level test set; (iii) potential slice-level split leakage if scans contribute slices to both train and val; and (iv) CycleGAN training capped at 2000 images. Prior work suggests that appearance-level synthesis may not fully model geometric variability critical for precise localization~\cite{hammami2020cyclegan}, and anatomically consistent volumetric synthesis (e.g., diffusion or 3D style-based methods) could be promising directions~\cite{khader2023ddpm3d,ellis2022stylegan3d}.
