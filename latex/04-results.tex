\section{Results}
This section summarizes the qualitative examples and quantitative performance of the four training settings and show the effects of CycleGAN and classical augmentation on object detection.

\subsection{Qualitative Examples}
Figure~\ref{fig:aug_examples} shows appearance-level augmentations (contrast, brightness, blur, noise, CLAHE) applied to a representative slice; Figure~\ref{fig:cyclegan_example} shows a CycleGAN-generated sample.

\begin{figure}[H]
\centering
\subfloat[Original]{%
  \includegraphics[width=0.3\linewidth]{figs/1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405_0142.png}
}\hfill
\subfloat[Contrast]{%
  \includegraphics[width=0.3\linewidth]{figs/1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405_0142_aug1.png}
}\hfill
\subfloat[Brightness]{%
  \includegraphics[width=0.3\linewidth]{figs/1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405_0142_aug2.png}
}

\vspace{0.6em}

\subfloat[Gaussian blur]{%
  \includegraphics[width=0.3\linewidth]{figs/1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405_0142_aug3.png}
}\hfill
\subfloat[Gaussian noise]{%
  \includegraphics[width=0.3\linewidth]{figs/1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405_0142_aug4.png}
}\hfill
\subfloat[CLAHE]{%
  \includegraphics[width=0.3\linewidth]{figs/1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405_0142_aug5.png}
}

\caption{Original CT slices including appearance-level augmentations.}
\label{fig:aug_examples}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Fake A]{%
  \includegraphics[width=0.3\linewidth]{figs/cyc_1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405_0142_fake_A.png}
}\hspace{0.05\linewidth}
\subfloat[Fake B]{%
  \includegraphics[width=0.3\linewidth]{figs/cyc_1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405_0142_fake_B.png}
}

\caption{CycleGAN-generated CT slices.}
\label{fig:cyclegan_example}
\end{figure}


\subsection{Quantitative Performance}
Four settings are evaluated on the fixed validation split using AP@0.5, AP@0.5:0.95, and Mean IoU.
\begin{table}[H]
\centering
\caption{Validation performance and data composition per training setting. \\
 $N_{\text{real}}$ = number of real training images (unique). \\
 $N_{\text{aug}}$ = number of classical augmentation copies. \\
 $N_{\text{gan}}$ = number of CycleGAN images.}
\begin{tabular}{lrrrrrr}
\toprule
Setting & $N_{\text{real}}$ & $N_{\text{aug}}$ & $N_{\text{gan}}$ & AP@0.5 & AP@0.5:0.95 & Mean IoU \\
\midrule
Baseline & 295 & 0 & 0 & 0.711 & 0.384 & 0.695 \\
Baseline + Classical Aug & 295 & $5\times$295 & 0 & 0.753 & 0.431 & 0.761 \\
Baseline + CycleGAN & 295 & 0 & 100 & 0.724 & 0.383 & 0.708 \\
Classical Aug + CycleGAN & 295 & $5\times$295 & 100 & \textbf{0.822} & \textbf{0.476} & \textbf{0.763} \\
\bottomrule
\end{tabular}
\end{table}
\noindent
Across all four training settings, classical augmentations consistently improved all metrics relative to the baseline trained solely on real slices. In particular, adding classical augmentations led to higher AP@0.5, AP@0.5:0.95, and Mean IoU. \\
Training with CycleGAN-generated images (in addition to real images) yielded a higher AP@0.5 (0.724 vs. 0.711) and Mean IoU (0.708 vs. 0.695) compared to baseline, while AP@0.5:0.95 decreased (0.383 vs. 0.384), although this difference is so small that it likely falls within normal variation. \\
The combined setting of classical augmentation + CycleGAN achieved the best overall performance across all metrics, though the gains over classical augmentation alone were modest. \\
Altogether, these results suggest that CycleGAN serves as a complementary source of appearance variability that can modestly improve performance but does not replace classical augmentation. \\
Given that classical augmentation also improved all metrics at lower computational overhead, the additional computational complexity of CycleGAN is hard to justify if solely used for appearance robustness.